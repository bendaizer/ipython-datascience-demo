{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Nous allons faire une petite d\u00e9monstration de traitement de texte. \n",
      "\n",
      "Les donn\u00e9es de Konbini dans mongodb."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import pymongo\n",
      "from pymongo import MongoClient\n",
      "from datetime import datetime"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import logging\n",
      "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.ERROR)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "KONBINIDIR=\"/Users/abenhenni/Work/konbini/\""
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 3
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "##Configure mongo"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "mongo_server = 'localhost'\n",
      "mongo_port = 27017\n",
      "client = MongoClient(mongo_server, mongo_port)\n",
      "#print client.database_names()\n",
      "konbini = client.konbini\n",
      "post_content = 'post_content_raw'"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 4
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Corpus preparation"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "La pr\u00e9paration d'un corpus de texte pour analyse se d\u00e9compose en diff\u00e9rentes \u00e9tapes :\n",
      "\n",
      "* tout d'abord on va enlever un certain nombre de mots qui ne font qu'ajouter du bruit : les STOPWORDS\n",
      "\n",
      "nltk dispose d'une liste de stopwords pour diff\u00e9rentes langues."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from nltk.stem import PorterStemmer\n",
      "\n",
      "stemmer = PorterStemmer()\n",
      "stemmer.stem('eat')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 12,
       "text": [
        "'eat'"
       ]
      }
     ],
     "prompt_number": 12
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def preprocess(text):\n",
      "    import nltk\n",
      "    import re\n",
      "    from nltk.corpus import stopwords\n",
      "    from nltk.stem import PorterStemmer\n",
      "\n",
      "    stops = unicode(stopwords.words('english'))\n",
      "    from nltk.tokenize.regexp import RegexpTokenizer\n",
      "    reg_words = r'''(?x)\n",
      "      # abbreviations, e.g. U.S.A. (with optional last period)\n",
      "      ([A-Z])(\\.[A-Z])+\\.?\n",
      "      # words with optional internal hyphens\n",
      "      | \\w+(-\\w+)*\n",
      "      # currency and percentages, e.g. $12.40, 82%\n",
      "      | \\$?\\d+(\\.\\d+)?%?\n",
      "    '''\n",
      "    tokenizer = RegexpTokenizer(reg_words, flags=re.UNICODE|re.IGNORECASE)\n",
      "    stemmer = PorterStemmer()\n",
      "    return [stemmer.stem(item) for item in tokenizer.tokenize(text) if item not in stops]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 7
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import gensim\n",
      "from gensim import utils\n",
      "from simserver import SessionServer"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 5
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "\n",
      "#precorpus = konbini.posts.find({'eligible':True },{\"post_id\",post_content}).limit(100)\n",
      "precorpus = konbini.posts.find({'post_locale': 'en'},{\"post_id\",post_content,\"post_title\"})\n",
      "corpus = [{'id':str(item['post_id']),'tokens':preprocess(item[post_content]+\" \"+item['post_title'])} \n",
      "          for item in list(precorpus)]\n",
      "print len(corpus)\n",
      "print corpus[0]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "531\n",
        "{'tokens': [u'Chanc', u'probabl', u'haven', u'heard', u'PetitTub', u'Don', u'worri', u'miss', u'anyth', u'except', u'potenti', u'massiv', u'giggl', u'less', u'success', u'YouTub', u'video', u'PetitTub', u'like', u'onlin', u'den', u'failur', u'Everyth', u'doesn', u'make', u'1', u'view', u'statu', u'YouTub', u'end', u'giant', u'wast', u'heap', u'viral', u'trash', u'It', u'pretti', u'hilari', u'scroll', u'YouTub', u'video', u'know', u'probabl', u'person', u'ever', u'see', u'With', u'hundr', u'thousand', u'minut', u'video', u'upload', u'YouTub', u'everi', u'day', u'pretti', u'obviou', u'see', u'footag', u'slip', u'crack', u'Thi', u'select', u'random', u'clip', u'like', u'graveyard', u'unpopular', u'video', u'site', u'would', u'go', u'qualiti', u'content', u'By', u'click', u'next', u'button', u'new', u'random', u'clip', u'made', u'avail', u'see', u'Now', u'thank', u'PetitTub', u'final', u'get', u'look', u'strang', u'clip', u'bad', u'nobodi', u'ever', u'actual', u'watch', u'Stuff', u'like', u'Priceless', u'Enjoy', u'PetitTub', u'Where', u'Youtub', u'Video', u'Go', u'To', u'Die'], 'id': '59233'}\n"
       ]
      }
     ],
     "prompt_number": 10
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "##similarity server start"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "service = SessionServer(KONBINIDIR+\"data/test_gensim_server\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "##First Corpus"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "initialization of server, nothing is indexed, model training with limited corpus"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "service.train(corpus, method='lsi');"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "there are no keys in the index after initial training"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "len(service.keys())"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "we need to index the corpus"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "service.index(corpus);"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "we can see that there are keys now"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "service.keys()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "##querying with indexed corpus and same training corpus"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import json\n",
      "json.dumps({item[0]:item[1] for item in reco})\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# TESTS"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "article = 88657\n",
      "item = konbini.posts.find_one({'post_id':article},{\"post_id\",'post_content_clean'})\n",
      "print {'id':str(item['post_id']),'tokens':preprocess(item['post_content_clean'])}"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "precorpus = konbini.posts.find({'post_id':article})\n",
      "corpus = [{'id':str(item['post_id']),'tokens':preprocess(item[post_content])} \n",
      "          for item in list(precorpus)]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "service.find_similar('88657')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}